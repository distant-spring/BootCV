% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Boot.CV.R
\name{Boot.CV}
\alias{Boot.CV}
\title{Bootstrap Cross-Validation}
\usage{
Boot.CV(data, Loss, m, B.bt = 400, B.cv = 20, lambda0 = 0.368)
}
\arguments{
\item{data}{data matrix of dimension nobs x nvars; each row is an observation vector.}

\item{Loss}{loss function which returns the summary statistics L with two inputs, train.data and test.data,
which are in the same form as data; it evaluates the performance of estimation fitted with train.data in test.data.}

\item{m}{training set size.}

\item{B.bt}{the number of bootstraps (default is 400).}

\item{B.cv}{the number of cross-validations (default is 20).}

\item{lambda0}{tuning parameter in determining the adjusted sample size of training set (default is 0.368).}
}
\value{
\item{sigma}{the bootstrap standard error of cross-validation estimator.}
\item{factor}{inflation factor to account for the reduced sample size in bootstrapped training set.}
}
\description{
Boot.CV implements the proposed algorithm and returns the bootstrap standard error
of cross-validation estimator and inflation factor.
}
\details{
The algorithm quickly estimates the standard error of cross-validation estimate
\deqn{\widehat{Err}^{CV}=\frac{1}{B_{CV}}\sum_{b=1}^{B_{CV}} L\left\{D_{test}^b, \hat{\psi}(D_{train}^b)\right\},}{}
where data \eqn{D=D_{train}^b\cup D_{test}^b} represents the b-th split in cross-validation and
loss function L evaluates the performance of estimation $\hat{\psi}$ fitted with train.data in test.data.
The adjusted sample size of training set \eqn{m_{adj}} is determined by minimizing
\eqn{(\frac{m_{adj}}{m/0.632}-1)^2+\lambda_0 (\frac{n-m}{n-m_{adj}}-1)^2.}
}
\examples{
library(lme4)

set.seed(1)
# data generation
n=90
p=10
x=matrix(rnorm(n*p), ncol=p)
beta=rnorm(p)
y=x\%*\%beta+rnorm(n)
data=cbind(y,x)

m=50 # training set size
# loss function
Loss=function(train_data,test_data){
  y=train_data[,1]
  x=train_data[,-1]
  yt=test_data[,1]
  xt=test_data[,-1]

  fit=lm(y~x)
  beta=fit$coef

  return(mean((yt-cbind(1,xt)\%*\%beta)^2))
}

boot=Boot_CV(data,Loss,m)
result1=CV_confint(boot,data,Loss,m,Method='Boot_CV',adj=T,print=T)
result2=CV_confint(boot,data,Loss,m,Method='Boot_CV',adj=F,print=T)
}
